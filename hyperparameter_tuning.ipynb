{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr8xZGyNU4IU",
        "outputId": "86137d03-8dc5-415a-fb1b-0c5fd3f9115e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def load_images_from_path(path, label):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for file in os.listdir(path):\n",
        "        # Check if the file is an image file\n",
        "        if file.endswith(('.jpg', '.jpeg', '.png')):  # Add other image extensions if needed\n",
        "            img = image.load_img(os.path.join(path, file), target_size=(224, 224, 3))\n",
        "            images.append(image.img_to_array(img))\n",
        "            labels.append((label))\n",
        "\n",
        "    # Print a message if no images were found\n",
        "    if not images:\n",
        "        print(f\"No images found in the directory: {path}\")\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "%matplotlib inline\n",
        "\n",
        "# ... (rest of the code remains the same)\n",
        "\n",
        "def show_images(images, grid_size=(4, 4)):  # Default grid size is 4x4\n",
        "    \"\"\"\n",
        "    Displays images in a grid format.\n",
        "\n",
        "    Args:\n",
        "        images: A list of images (NumPy arrays).\n",
        "        grid_size: A tuple (rows, cols) specifying the grid dimensions.\n",
        "    \"\"\"\n",
        "    num_images = len(images)\n",
        "\n",
        "    # Handle the case where there are no images\n",
        "    if num_images == 0:\n",
        "        print(\"No images to display.\")\n",
        "        return\n",
        "\n",
        "    # Calculate grid dimensions based on the number of images and grid size\n",
        "    num_rows = grid_size[0]\n",
        "    num_cols = grid_size[1]\n",
        "    num_images_to_display = min(num_images, num_rows * num_cols)\n",
        "\n",
        "    # Create subplots with the calculated grid dimensions\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20), subplot_kw={'xticks': [], 'yticks': []})\n",
        "\n",
        "    # Flatten the axes array for easier iteration\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Display images on subplots\n",
        "    for i in range(num_images_to_display):\n",
        "        axes[i].imshow(images[i] / 255)\n",
        "\n",
        "    # Hide any extra subplots if there are fewer images than grid cells\n",
        "    for i in range(num_images_to_display, num_rows * num_cols):\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()  # Adjust spacing between subplots\n",
        "    plt.show()\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = []"
      ],
      "metadata": {
        "id": "oUrqg0xNU42Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = load_images_from_path('/content/drive/MyDrive/data/train/arctic_fox', 0)\n",
        "x_train += images\n",
        "y_train += labels"
      ],
      "metadata": {
        "id": "VdaodUTgVEv3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = load_images_from_path('/content/drive/MyDrive/data/train/polar_bear', 1)\n",
        "\n",
        "x_train += images\n",
        "y_train += labels"
      ],
      "metadata": {
        "id": "40NkhGn_VJYV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = load_images_from_path('/content/drive/MyDrive/data/train/walrus', 2)\n",
        "\n",
        "\n",
        "x_train += images\n",
        "y_train += labels"
      ],
      "metadata": {
        "id": "21LAvFHEVLjO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = load_images_from_path('/content/drive/MyDrive/data/test/arctic_fox', 0)\n",
        "\n",
        "x_test += images\n",
        "y_test += labels"
      ],
      "metadata": {
        "id": "ZH51xq3oVSOn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = load_images_from_path('/content/drive/MyDrive/data/test/polar_bear', 1)\n",
        "\n",
        "x_test += images\n",
        "y_test += labels"
      ],
      "metadata": {
        "id": "SKR1CbrLVV-4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = load_images_from_path('/content/drive/MyDrive/data/test/walrus', 2)\n",
        "\n",
        "x_test += images\n",
        "y_test += labels"
      ],
      "metadata": {
        "id": "Pxj9MGmGVY43"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "x_train = np.array(x_train) / 255\n",
        "x_test = np.array(x_test) / 255\n",
        "\n",
        "y_train_encoded = to_categorical(y_train)\n",
        "y_test_encoded = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "hs89gBzkVcuW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 1: Define a function to create a CNN model with specific hyperparameters\n",
        "from tensorflow import keras\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout # Import Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def create_tuned_cnn(kernel_size, num_filters, learning_rate, dropout_rate, dense_units, activation):\n",
        "    tuned_model = Sequential([\n",
        "        Conv2D(num_filters, kernel_size, activation=activation, input_shape=x_train.shape[1:]),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(dense_units, activation=activation),\n",
        "        Dropout(dropout_rate),\n",
        "        Dense(y_train_encoded.shape[1], activation='softmax')  # Adjust output layer for number of classes\n",
        "    ])\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    tuned_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return tuned_model"
      ],
      "metadata": {
        "id": "K3O5EWiWVdOu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 2: Define ranges for hyperparameters\n",
        "from random import randint, uniform, choice\n",
        "\n",
        "num_trials = 5 # Number of random combinations to try\n",
        "\n",
        "# Store results for each trial\n",
        "results = []\n"
      ],
      "metadata": {
        "id": "0qu7uE2UVikF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 3: Train and evaluate models with random hyperparameters\n",
        "for trial in range(num_trials):\n",
        "    print(f\"Trial {trial + 1}/{num_trials}\")\n",
        "\n",
        "    # Randomly select hyperparameters\n",
        "    kernel_size = randint(2, 5)  # Random kernel size between 2 and 5\n",
        "    num_filters = randint(16, 64)  # Random number of filters between 16 and 64\n",
        "    learning_rate = round(uniform(0.0001, 0.01), 5)  # Random learning rate between 0.0001 and 0.01\n",
        "    dropout_rate = round(uniform(0.2, 0.5), 2)  # Random dropout rate between 0.2 and 0.5\n",
        "    dense_units = randint(64, 256)  # Random number of neurons in the dense layer\n",
        "    activation = choice(['relu', 'tanh'])  # Random activation function\n",
        "\n",
        "    print(f\"Testing with kernel_size={kernel_size}, num_filters={num_filters}, learning_rate={learning_rate}, \"\n",
        "          f\"dropout_rate={dropout_rate}, dense_units={dense_units}, activation={activation}\")\n",
        "\n",
        "    # Create and train the model\n",
        "    tuned_model = create_tuned_cnn((kernel_size, kernel_size), num_filters, learning_rate, dropout_rate, dense_units, activation)\n",
        "    history = tuned_model.fit(x_train, y_train_encoded, epochs=5, batch_size=randint(32, 128), validation_data=(x_test, y_test_encoded), verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = tuned_model.predict(x_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true = np.argmax(y_test_encoded, axis=1)\n",
        "\n",
        "    # Compute classification metrics\n",
        "    from sklearn.metrics import classification_report\n",
        "    report = classification_report(y_true, y_pred_classes, output_dict=True)\n",
        "    accuracy = report['accuracy']\n",
        "    precision = np.mean([v['precision'] for k, v in report.items() if k.isdigit()])\n",
        "    recall = np.mean([v['recall'] for k, v in report.items() if k.isdigit()])\n",
        "    f1_score = np.mean([v['f1-score'] for k, v in report.items() if k.isdigit()])\n",
        "\n",
        "    # Store trial results\n",
        "    results.append({\n",
        "        'kernel_size': kernel_size,\n",
        "        'num_filters': num_filters,\n",
        "        'learning_rate': learning_rate,\n",
        "        'dropout_rate': dropout_rate,\n",
        "        'dense_units': dense_units,\n",
        "        'activation': activation,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1_score\n",
        "    })\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS4Sj2afVkmI",
        "outputId": "116df29b-d105-47e1-c449-ce207bd5ddfb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 1/5\n",
            "Testing with kernel_size=4, num_filters=35, learning_rate=0.00017, dropout_rate=0.31, dense_units=155, activation=tanh\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 845ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.3333, Precision: 0.1111, Recall: 0.3333, F1-Score: 0.1667\n",
            "Trial 2/5\n",
            "Testing with kernel_size=2, num_filters=47, learning_rate=0.00618, dropout_rate=0.23, dense_units=152, activation=relu\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 529ms/step\n",
            "Accuracy: 0.3667, Precision: 0.2404, Recall: 0.3667, F1-Score: 0.2525\n",
            "Trial 3/5\n",
            "Testing with kernel_size=4, num_filters=28, learning_rate=0.00053, dropout_rate=0.3, dense_units=169, activation=tanh\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ab97e984af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 454ms/step"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ab97e984af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 437ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3333, Precision: 0.1111, Recall: 0.3333, F1-Score: 0.1667\n",
            "Trial 4/5\n",
            "Testing with kernel_size=5, num_filters=26, learning_rate=0.00462, dropout_rate=0.34, dense_units=146, activation=tanh\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 757ms/step\n",
            "Accuracy: 0.3333, Precision: 0.1111, Recall: 0.3333, F1-Score: 0.1667\n",
            "Trial 5/5\n",
            "Testing with kernel_size=3, num_filters=34, learning_rate=0.00942, dropout_rate=0.42, dense_units=73, activation=tanh\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 683ms/step\n",
            "Accuracy: 0.3333, Precision: 0.1111, Recall: 0.3333, F1-Score: 0.1667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 4: Find and display the best hyperparameter combination\n",
        "best_result = max(results, key=lambda x: x['accuracy'])\n",
        "# Stage 4: Find and display the best hyperparameter combination\n",
        "best_result = max(results, key=lambda x: x['accuracy'])\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(best_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va19aD4lVmXN",
        "outputId": "cff0d9d5-10d1-4645-a6b3-a79e75ccda35"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Hyperparameters:\n",
            "{'kernel_size': 2, 'num_filters': 47, 'learning_rate': 0.00618, 'dropout_rate': 0.23, 'dense_units': 152, 'activation': 'relu', 'accuracy': 0.36666666666666664, 'precision': 0.2403628117913832, 'recall': 0.36666666666666664, 'f1_score': 0.25254039646843246}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract best hyperparameters\n",
        "best_hyperparams = max(results, key=lambda x: x['accuracy'])\n",
        "print(\"\\nRetraining model with best hyperparameters:\")\n",
        "print(best_hyperparams)\n",
        "\n",
        "# Create and retrain model with best hyperparameters\n",
        "final_model = create_tuned_cnn(\n",
        "    kernel_size=(best_hyperparams['kernel_size'], best_hyperparams['kernel_size']),\n",
        "    num_filters=best_hyperparams['num_filters'],\n",
        "    learning_rate=best_hyperparams['learning_rate'],\n",
        "    dropout_rate=best_hyperparams['dropout_rate'],\n",
        "    dense_units=best_hyperparams['dense_units'],\n",
        "    activation=best_hyperparams['activation']\n",
        ")\n",
        "\n",
        "# Train the final model\n",
        "history = final_model.fit(\n",
        "    x_train, y_train_encoded,\n",
        "    epochs=10,  # Adjust epochs for full convergence\n",
        "    batch_size=64,  # You can use an optimal batch size\n",
        "    validation_data=(x_test, y_test_encoded),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Print the training accuracy after each epoch\n",
        "for epoch in range(len(history.history['accuracy'])):\n",
        "    print(f\"Epoch {epoch + 1}/{len(history.history['accuracy'])} - \"\n",
        "          f\"Accuracy: {history.history['accuracy'][epoch]:.4f} \"\n",
        "          f\"Loss: {history.history['loss'][epoch]:.4f}\")\n",
        "\n",
        "# Evaluate the final model on the test set\n",
        "y_pred = final_model.predict(x_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test_encoded, axis=1)\n",
        "\n",
        "# Compute and compare metrics\n",
        "from sklearn.metrics import classification_report\n",
        "final_report = classification_report(y_true, y_pred_classes, output_dict=True)\n",
        "\n",
        "# Print final model performance with training accuracy\n",
        "final_training_accuracy = history.history['accuracy'][-1]  # Last training accuracy\n",
        "\n",
        "print(\"\\nFinal Model Performance:\")\n",
        "accuracy = final_report['accuracy']\n",
        "precision = np.mean([v['precision'] for k, v in final_report.items() if k.isdigit()])\n",
        "recall = np.mean([v['recall'] for k, v in final_report.items() if k.isdigit()])\n",
        "f1_score = np.mean([v['f1-score'] for k, v in final_report.items() if k.isdigit()])\n",
        "\n",
        "print(f\"Training Accuracy: {final_training_accuracy:.4f}\")  # Print training accuracy\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYbLtTezVpGx",
        "outputId": "fe51fdad-f64f-4dd2-e380-b3de75450298"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Retraining model with best hyperparameters:\n",
            "{'kernel_size': 2, 'num_filters': 47, 'learning_rate': 0.00618, 'dropout_rate': 0.23, 'dense_units': 152, 'activation': 'relu', 'accuracy': 0.36666666666666664, 'precision': 0.2403628117913832, 'recall': 0.36666666666666664, 'f1_score': 0.25254039646843246}\n",
            "Epoch 1/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 6s/step - accuracy: 0.3797 - loss: 244.3294 - val_accuracy: 0.3333 - val_loss: 88.0565\n",
            "Epoch 2/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 6s/step - accuracy: 0.2945 - loss: 57.1998 - val_accuracy: 0.4833 - val_loss: 2.8128\n",
            "Epoch 3/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 6s/step - accuracy: 0.4408 - loss: 2.3317 - val_accuracy: 0.3667 - val_loss: 1.0719\n",
            "Epoch 4/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5s/step - accuracy: 0.6188 - loss: 0.8285 - val_accuracy: 0.5083 - val_loss: 0.9938\n",
            "Epoch 5/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 6s/step - accuracy: 0.8177 - loss: 0.7397 - val_accuracy: 0.5583 - val_loss: 1.0342\n",
            "Epoch 6/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - accuracy: 0.8891 - loss: 0.5037 - val_accuracy: 0.5083 - val_loss: 1.5061\n",
            "Epoch 7/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 5s/step - accuracy: 0.9260 - loss: 0.3103 - val_accuracy: 0.5083 - val_loss: 1.2167\n",
            "Epoch 8/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7s/step - accuracy: 0.9838 - loss: 0.1953 - val_accuracy: 0.5000 - val_loss: 1.8834\n",
            "Epoch 9/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 6s/step - accuracy: 0.9893 - loss: 0.1125 - val_accuracy: 0.5000 - val_loss: 1.8625\n",
            "Epoch 10/10\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 5s/step - accuracy: 0.9830 - loss: 0.0752 - val_accuracy: 0.4250 - val_loss: 2.7683\n",
            "Epoch 1/10 - Accuracy: 0.3733 Loss: 279.3857\n",
            "Epoch 2/10 - Accuracy: 0.3133 Loss: 40.8231\n",
            "Epoch 3/10 - Accuracy: 0.4500 Loss: 1.7783\n",
            "Epoch 4/10 - Accuracy: 0.6533 Loss: 0.8465\n",
            "Epoch 5/10 - Accuracy: 0.8067 Loss: 0.7065\n",
            "Epoch 6/10 - Accuracy: 0.8900 Loss: 0.4804\n",
            "Epoch 7/10 - Accuracy: 0.9433 Loss: 0.3079\n",
            "Epoch 8/10 - Accuracy: 0.9833 Loss: 0.1871\n",
            "Epoch 9/10 - Accuracy: 0.9867 Loss: 0.1091\n",
            "Epoch 10/10 - Accuracy: 0.9900 Loss: 0.0709\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 515ms/step\n",
            "\n",
            "Final Model Performance:\n",
            "Training Accuracy: 0.9900\n",
            "Test Accuracy: 0.4250\n",
            "Precision: 0.3900\n",
            "Recall: 0.4250\n",
            "F1-Score: 0.3666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xbZkeBypgZ0F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}